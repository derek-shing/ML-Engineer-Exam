Creating a Streaming Data Pipeline for a Real-Time Dashboard with Dataflow
1 hour
Free
Overview
In this lab, you own a fleet of New York City taxi cabs and are looking to monitor how well your business is doing in real-time. You build a streaming data pipeline to capture taxi revenue, passenger count, ride status, and much more, and then visualize the results in a management dashboard.

Objectives
In this lab you learn how to:

Create a Dataflow job from a template

Subscribe to a Pub/Sub topic

Stream a Dataflow pipeline into BigQuery

Monitor a Dataflow pipeline in BigQuery

Analyze results with SQL

Visualize key metrics in Looker Studio

Set up and requirements
Before you click the Start Lab button
Note: Read these instructions.
Labs are timed and you cannot pause them. The timer, which starts when you click Start Lab, shows how long Google Cloud resources will be made available to you.
This Qwiklabs hands-on lab lets you do the lab activities yourself in a real cloud environment, not in a simulation or demo environment. It does so by giving you new, temporary credentials that you use to sign in and access Google Cloud for the duration of the lab.

What you need
To complete this lab, you need:

Access to a standard internet browser (Chrome browser recommended).
Time to complete the lab.
Note: If you already have your own personal Google Cloud account or project, do not use it for this lab.
Note: If you are using a Pixelbook, open an Incognito window to run this lab.
How to start your lab and sign in to the Console
Click the Start Lab button. If you need to pay for the lab, a pop-up opens for you to select your payment method. On the left is a panel populated with the temporary credentials that you must use for this lab.

Credentials panel

Copy the username, and then click Open Google Console. The lab spins up resources, and then opens another tab that shows the Choose an account page.

Note: Open the tabs in separate windows, side-by-side.
On the Choose an account page, click Use Another Account. The Sign in page opens.

Choose an account dialog box with Use Another Account option highlighted

Paste the username that you copied from the Connection Details panel. Then copy and paste the password.

Note: You must use the credentials from the Connection Details panel. Do not use your Google Cloud Skills Boost credentials. If you have your own Google Cloud account, do not use it for this lab (avoids incurring charges).
Click through the subsequent pages:
Accept the terms and conditions.
Do not add recovery options or two-factor authentication (because this is a temporary account).
Do not sign up for free trials.
After a few moments, the Cloud console opens in this tab.

Note: You can view the menu with a list of Google Cloud Products and Services by clicking the Navigation menu at the top-left. Cloud Console Menu
Activate Google Cloud Shell
Google Cloud Shell is a virtual machine that is loaded with development tools. It offers a persistent 5GB home directory and runs on the Google Cloud.

Google Cloud Shell provides command-line access to your Google Cloud resources.

In Cloud console, on the top right toolbar, click the Open Cloud Shell button.

Highlighted Cloud Shell icon

Click Continue.

It takes a few moments to provision and connect to the environment. When you are connected, you are already authenticated, and the project is set to your PROJECT_ID. For example:

Project ID highlighted in the Cloud Shell Terminal

gcloud is the command-line tool for Google Cloud. It comes pre-installed on Cloud Shell and supports tab-completion.

You can list the active account name with this command:
gcloud auth list
Copied!
Output:

Credentialed accounts:
 - @.com (active)
Example output:

Credentialed accounts:
 - google1623327_student@qwiklabs.net
You can list the project ID with this command:
gcloud config list project
Copied!
Output:

[core]
project =
Example output:

[core]
project = qwiklabs-gcp-44776a13dea667a6
Note: Full documentation of gcloud is available in the gcloud CLI overview guide .
Task 1. Source a pre-created Pub/Sub topic and create a BigQuery dataset
In this task, you create the taxirides dataset. You have two different options which you can use to create this, using the Google Cloud Shell or the Google Cloud Console.

Pub/Sub is an asynchronous global messaging service. By decoupling senders and receivers, it allows for secure and highly available communication between independently written applications. Pub/Sub delivers low-latency, durable messaging.

In Pub/Sub, publisher applications and subscriber applications connect with one another through the use of a shared string called a topic. A publisher application creates and sends messages to a topic. Subscriber applications create a subscription to a topic to receive messages from it.

Google maintains a few public Pub/Sub streaming data topics for labs like this one. We'll be using an extract of the NYC Taxi & Limousine Commissionâ€™s open dataset. The Pub/Sub topic has already been created and populated in your project environment.

BigQuery is a serverless data warehouse. Tables in BigQuery are organized into datasets. In this lab, messages published into Pub/Sub will be aggregated and stored in BigQuery.

Use one of the following options to create a new BigQuery dataset:

Option 1: The command-line tool
In Cloud Shell (Cloud Shell icon), run the following command to create the taxirides dataset.

bq --location=us-west1 mk taxirides
Copied!
Run this command to create the taxirides.realtime table (empty schema that you will stream into later).

bq --location=us-west1 mk \
--time_partitioning_field timestamp \
--schema ride_id:string,point_idx:integer,latitude:float,longitude:float,\
timestamp:timestamp,meter_reading:float,meter_increment:float,ride_status:string,\
passenger_count:integer -t taxirides.realtime
Copied!
Option 2: The BigQuery Console UI
Note: Skip these steps if you created the tables using the command line.
In the Google Cloud console, in the Navigation menu(Navigation Menu), click BigQuery.

If you see the Welcome dialog, click Done.

Click on View actions (View Actions) next to your Project ID, and then click Create dataset.

In Dataset ID, type taxirides.

In Data location, click us-west1 (Oregon), and then click Create Dataset.

In the Explorer pane, click expand node (Expander) to reveal the new taxirides dataset.

Click on View actions (View Actions) next to the taxirides dataset, and then click Open.

Click Create Table.

In Table, type realtime

For the schema, click Edit as text and paste in the following:

ride_id:string,
point_idx:integer,
latitude:float,
longitude:float,
timestamp:timestamp,
meter_reading:float,
meter_increment:float,
ride_status:string,
passenger_count:integer
Copied!
In Partition and cluster settings, select timestamp.

Click Create Table.

Task 2. Create a Cloud Storage bucket
In this task, you create a Cloud Storage bucket to provide working space for your Dataflow pipeline.

Cloud Storage allows world-wide storage and retrieval of any amount of data at any time. You can use Cloud Storage for a range of scenarios including serving website content, storing data for archival and disaster recovery, or distributing large data objects to users via direct download.

In the Cloud console, in the Navigation menu (Navigation Menu), click Cloud Storage > Buckets.
Click Create Bucket.
For Name, copy and paste in your Project ID, and then click Continue.
For Location type, click Multi-region if it is not already selected.
Click Create.
Task 3. Set up a Dataflow Pipeline
In this task, you set up a streaming data pipeline to read sensor data from Pub/Sub, compute the maximum temperature within a time window, and write this out to BigQuery.

Dataflow is a serverless way to carry out data analysis.

Restart the connection to the Dataflow API.
In the Cloud console, in the top search bar, type Dataflow API, and then press ENTER.

In the search results window, click Dataflow API.

Click Manage.

Click Disable API.

In the Disable dialog, click Disable.

Click Enable.

Create a new streaming pipeline:
In the Cloud console, in the Navigation menu (Navigation Menu), click Dataflow.

In the top menu bar, click Create Job From Template.

Type streaming-taxi-pipeline as the Job name for your Dataflow job.

In Regional endpoint, select us-central1 (Iowa).

In Dataflow template, select the Pub/Sub Topic to BigQuery template.

In Input Pub/Sub topic, select the topic that already exists in your project from the dropdown list . It will appear like the following:

projects/<myprojectid>/topics/taxirides-realtime

In BigQuery output table, type <myprojectid>:taxirides.realtime

Note: You must replace myprojectid with your Project ID.
Note: There is a colon : between the project and dataset name and a dot . between the dataset and table name.
In Temporary location, click Browse.

Click view child resources(view child resources).

Click Create new folder(new folder), and then type the name tmp.

Click Create, and then click Select.

Click Show Optional Parameters.

In Max workers, type 2

In Number of workers, type 1

Click Run Job.

A new streaming job has started! You can now see a visual representation of the data pipeline. It will take 3 to 5 minutes for data to begin moving into BigQuery.

Note: If the dataflow job fails for the first time then re-create a new job template with new job name and run the job.
Task 4. Analyze the taxi data using BigQuery
In this task, you analyze the data as it is streaming.

In the Cloud console, in the Navigation menu (Navigation Menu), click BigQuery.

If the Welcome dialog appears, click Done.

In the Query Editor, type the following, and then click Run:

SELECT * FROM taxirides.realtime LIMIT 10
Copied!
Note: If no records are returned, wait another minute and re-run the above query (Dataflow takes 3-5 minutes to setup the stream).
Your output will look similar to the following: Sample BigQuery output from query

Task 5. Perform aggregations on the stream for reporting
In this task, you calculate aggregations on the stream for reporting.

In the Query Editor, clear the current query.

Copy and paste the following query, and then click Run.

WITH streaming_data AS (
SELECT
  timestamp,
  TIMESTAMP_TRUNC(timestamp, HOUR, 'UTC') AS hour,
  TIMESTAMP_TRUNC(timestamp, MINUTE, 'UTC') AS minute,
  TIMESTAMP_TRUNC(timestamp, SECOND, 'UTC') AS second,
  ride_id,
  latitude,
  longitude,
  meter_reading,
  ride_status,
  passenger_count
FROM
  taxirides.realtime
ORDER BY timestamp DESC
LIMIT 1000
)
# calculate aggregations on stream for reporting:
SELECT
 ROW_NUMBER() OVER() AS dashboard_sort,
 minute,
 COUNT(DISTINCT ride_id) AS total_rides,
 SUM(meter_reading) AS total_revenue,
 SUM(passenger_count) AS total_passengers
FROM streaming_data
GROUP BY minute, timestamp
Copied!
Note: Ensure Dataflow is registering data in BigQuery before proceeding to the next task.
The result shows key metrics by the minute for every taxi drop-off.

Click Save > Save query.

In the Save query dialog, in the Name field, type My Saved Query.

Click Save.

Task 6. Stop the Dataflow Job
In this task, you stop the Dataflow job to free up resources for your project.

In the Cloud console, in the Navigation menu (Navigation Menu), click Dataflow.

Click the streaming-taxi-pipeline, or the new job name.

Click Stop, and then select Cancel > Stop Job.

Task 7. Create a real-time dashboard
In this task, you create a real-time dashboard to visualize the data.

In the Cloud console, in the Navigation menu (Navigation Menu), click BigQuery.

In the Explorer Pane, expand your Project ID.

Expand Saved queries, and then click My Saved Query.

Your query is loaded in to the query editor.

Click Run.

In BigQuery, click Explore Data > Explore with Looker Studio.

Looker Studio Opens.

In the Looker Studio window, click your bar chart.

(Bar Chart

The Chart pane appears.

Click Add a chart, and then select Combo chart.

Combo chart

In the Setup pane, in Data Range Dimension, hover over minute (Date) and click X to remove it.

In the Data pane, click dashboard_sort and drag it to Setup > Data Range Dimension > Add dimension.

In Setup > Dimension, click minute, and then select dashboard_sort.

In Setup > Metric, click dashboard_sort, and then select total_rides.

In Setup > Metric, click Add metric, and then select total_passengers.

In Setup > Metric, click Record Count, and then select total_revenue.

In Setup > Sort, click total_rides, and then select dashboard_sort.

In Setup > Sort, click Ascending.

Your chart should look similar to this:

Sample chart

Note: Visualizing data at a minute-level granularity is currently not supported in Looker Studio as a timestamp. This is why we created our own dashboard_sort dimension.
When you're happy with your dashboard, click Save and share to save this data source.

If prompted to complete your account setup, agree to the terms and conditions, and then click Continue.

If prompted for email preferences, answer no to all, then click Continue.

If prompted with the Review data access before saving window, click Acknowledge and save.

Click Add to report.

Whenever anyone visits your dashboard, it will be up-to-date with the latest transactions. You can try it yourself by clicking More options (More Options), and then Refresh data.

Task 8. Create a time series dashboard
In this task, you create a time series chart.

Click this Looker Studio link to open Looker Studio in a new browser tab.

On the Reports page, in the Start with a Template section, click the [+] Blank Report template.

A new, empty report opens with the Add data to report window.

From the list of Google Connectors, select the BigQuery tile.

Click Custom Query, and then select your ProjectID. This should appear in the following format, qwiklabs-gcp-xxxxxxx.

In Enter Custom Query, paste the following query:

SELECT
  *
FROM
  taxirides.realtime
WHERE
  ride_status='dropoff'
Copied!
Click Add > Add To Report.

A new untitled report appears. It may take up to a minute for the screen to finish refreshing.

Create a time series chart
In the Data pane, click Add a Field.

Click All Fields on the left corner.

Change the timestamp field type to Date & Time > Date Hour Minute (YYYYMMDDhhmm).

In the change timestamp dialog, click Continue, and then click Done.

In the top menu, click Add a chart.

Choose Time series chart.

Time Series

Position the chart in the bottom left corner - in the blank space.

In Setup > Dimension, click timestamp (Date), and then select timestamp.

In Setup > Dimension, click timestamp, and then select calendar. Calendar

In Type, select Date & Time > Date Hour Minute.

Click outside the dialog to close it. You do not need to add a name.

In Setup > Metric, click Record Count, and then select meter reading.

Your time series chart should look similar to this but will vary based upon the volume and rate of data that was loaded from Pub/Sub:

Sample time series chart
